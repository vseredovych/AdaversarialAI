---------------------------------------------------------------------------------------

Отже темою моєї роботи була Аналіз атак на лінійні методи машинного навчання.
Під поняттям атаки на модель машинного навчання будемо розуміти генерування adversarial або в перекладі змагальних прикладів.
Отже нехай в нас є деякий класифікатор і приклад x. Тоді метою атаки на модель машинного навчання є знайти такий змагального прикладу x*, який буде знаходиться в деякому епсилон околі x, але буде хибно передбачатись класифікатором моделі. Рівень деформації зображення зазвичай контролюють за допомогою деякої норми. В роботі використовувалась норма L нескінченість.

Таким чином етапи роботи були наступні:
- Реалізувати лінійну модель машинного навчання та натренувати її для деякого датасету
- Розглянути деякі методи атак та захисту цієї моделі і проаналізувати їх ефектиність
---------------------------------------------------------------------------------------

В якості лінійного методу машинного навчання будемо використовувати мультикласову логістичну регресію. Активаційною функцію буде виступати softmax для того щоб передбачати декілька класів.
Для вагової функції використовуватимемо функцію кросс ентропії, а для того щоб мінімізувати цю функцію скористаємость алгоритмом градієнтного спуску.

---------------------------------------------------------------------------------------
Найпершим і найпоширенішим методом атак є Фаст Градієнт Сайн метод. Цей метод атак розрахований на відкриті моделі, тобто коли в нападника є доступ до параметрів моделі таких як зсув та ваги. Ідея методу полягає в тому щоб максимізувати функцію втрати до певного обмеження. Щоб досягти цього а алгоритм градієнтного спуску мають буди внесені зміни - тепер будуть знаходитись не оптимальні параметри моделі, а оптимальний X і до оригінального прикладу будемо додавати градіэнт від вагової функції по X. А для того щоб результат зберігався в деякому епсилон околі норму, використовується наступну clip функцію

---------------------------------------------------------------------------------------

Існує також ітераційний варіант цього методу. В ньому ми задаємо деяку кількість ітерації та норму. Далі беде вибраний деякий параметр alpha, і алгоритм буде поступово вносити зміни в оригінальне зображення, кожного разу перераховуючи градієнт. На виході отримаємо змагальний приклад.

---------------------------------------------------------------------------------------
Ще одним методом який розглядається в роботі є DeepFool. Якщо попередній алгоритм в першу чергу був націлений на швидке знаходження змагального прикладу, то це метод намагаэться знайти приклаж з мінімально можливими деформаціями оригінального зображення.
Для бінарного випадку якщо ми подивимось на площину гіперпараметру яка розділяє два класи, то очевидно що для того щоб приклад X був неправильно передбачений моделю достатньо зсунути його на відстань нормалі від цієї точки до межі рішень.

---------------------------------------------------------------------------------------
На цьому слайді можете бачити результата роботи алгоритміів...

---------------------------------------------------------------------------------------
Першим методом захисту який будемо зозглядати є так званий метод Рандомізації. 
Отже ми спочатку беремо вхідне зображення і випадково змінюємо його розмірність в певному малому околі. 
Далі ми заповнюємо деяку область навколо зображення нулями і тоді передаємо це зображення класифікатору. 
Ідея методу в тому щоб зруйнувати структуру змагального прикладу за допомогою випадковий змін зображеня.
---------------------------------------------------------------------------------------
Pixel Reflection


---------------------------------------------------------------------------------------
Attacks alalysis

---------------------------------------------------------------------------------------
Alalyzis Randomization


---------------------------------------------------------------------------------------
Alalyzis Pixel Deflection

---------------------------------------------------------------------------------------
Основна причина цього полягає в тому, що для того щоб захиститись від атак такими методами необхідно досить суттєво деформувати вхідне зображення, а через те що лінійні методи не здатні побудувати достатньо складні зв'язки між даними, вони є досить вразливі до такий деформацій.
Отже, точність передбачення на деформованих зображеннях може падати досить швидко і такі методи захисту не можуть бути використані належним чином.