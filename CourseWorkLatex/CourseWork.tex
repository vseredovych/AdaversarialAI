% -*- mode: LaTeX; coding: utf8; -*-

\documentclass[a4paper,14pt]{extreport}

% правильне кодування
\usepackage[utf8]{inputenc}
\usepackage[T2A]{fontenc}
\usepackage[ukrainian]{babel}

% починати абзаци невеликим відступом першого рядка
\usepackage{indentfirst}
\usepackage[pdftex,unicode,bookmarks]{hyperref}

\usepackage{color}
\definecolor{bluegray}{RGB}{230,230,255}
\usepackage{listings}

\usepackage{algorithmicx}

\lstset{
	extendedchars=\true, % дозволити кирилицю в лінстингу
	inputencoding=utf8,
	breaklines=true,
	basicstyle=\ttfamily,
	numbers=left,
	frame=single,
	backgroundcolor=\color{bluegray}
}

% якщо прийдеться вставляти код (простіше ніж listings, але немає breaklines)
\usepackage{verbatim}

% інтервал - півтора. 
\usepackage{setspace}
\onehalfspacing

% поля
\usepackage{geometry}
\geometry{a4paper}
\geometry{left=35mm,right=15mm,top=20mm,bottom=20mm}
\geometry{headheight=2ex,headsep=10mm,footskip=10mm}

%
\usepackage{mathtools}
\usepackage{amsmath}

%для картинок
\usepackage{graphicx}
\usepackage{caption}

% lstlisting settings
\usepackage{xcolor}

% для часткових похыдних
\usepackage{physics}


% для алгоритмів
%\usepackage[ruled,vlined]{algorithm2e}
\usepackage{algpseudocode}
\usepackage{algorithm}
\floatname{algorithm}{Algorithm}
\renewcommand{\algorithmicrequire}{\textbf{Input: }}
\renewcommand{\algorithmicensure}{\textbf{Output: }}
\newcommand{\algorithmreturn}{\textbf{return }}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
	backgroundcolor=\color{backcolour},   
	commentstyle=\color{codegreen},
	keywordstyle=\color{magenta},
	numberstyle=\tiny\color{codegray},
	stringstyle=\color{codepurple},
	basicstyle=\ttfamily\footnotesize,
	breakatwhitespace=false,         
	breaklines=true,                 
	captionpos=b,                    
	keepspaces=true,                 
	numbers=left,                    
	numbersep=5pt,                  
	showspaces=false,                
	showstringspaces=false,
	showtabs=false,                  
	tabsize=2
}
\lstset{style=mystyle}


% custom commands
\newcommand{\tran}{^{T}}
\newcommand{\ith}{^{(i)}}
%\newcommand{\someth}[1]{^{(#1)}}



\begin{document}
	% ============================================ %
	\begin{titlepage}%
    	\begin{center}
	    	{ЛЬВІВСЬКИЙ НАЦІОНАЛЬНИЙ УНІВЕРСИТЕТ \\ІМЕНІ ІВАНА ФРАНКА}\par
	       	{ФAКУЛЬТЕТ ПРИКЛАДНОЇ МАТЕМАТИКИ ТА ІНФОРМАТИКИ\\ КАФЕДРА ОБЧИСЛЮВАЛЬНОЇ МАТЕМАТИКИ}\par
			\begin{center}
	
	        \end{center}
	        \vspace{10mm}
	        \b{КУРСОВА РОБОТА}\par
	        {\small{на тему:}}\par
	        \vspace{20mm}
	        {\LARGE{\bf{\scshape{Аналіз атак на лінійні моделі машинного навчання}}}}\par
	        \vspace{5mm}
	        {}\par %subtitle
        \end{center}
	   	
	   	\vfill
	   	\hfill
		\begin{flushright}
   	   		\begin{minipage}[t]{80mm}
   	   			\flushright
	   	   		студента III курсу\\
	   	   		групи ПМп-31\\
	   	   		{Середовича Віктора}\par
	   	   		\vspace{2ex}
	   	   		Науковий керівник:\\
	   	   		{доцент Ю.А.Музичук}\\
   	   		\end{minipage}
   	   \end{flushright}
      
   	   \vspace{10mm}
   	   \begin{flushleft}
   	   \begin{minipage}[t]{80mm}
	   	   \flushleft
	   	   Завідуючий кафедри обчислювальної математики\\
	   	   проф.  
   	   \end{minipage}
   	   \end{flushleft}
   	   \vfill
   	   \vspace{10mm}
   	   
   	   \begin{center}Львів --- 2020\end{center}
   	   \stepcounter{page}
    \end{titlepage}
	% ============================================ %
	
	
	% ============================================ %
	\tableofcontents
	\newpage
	% ============================================ %
	
	
	% ============================================ %
	\chapter{Вступ}
	\textgreater\textbf{TODO}
	Машинне начання та штучний інтелект активно використовується у різних областях нашого життя, та допомагає у вирішенні таких задач як розпізнавання дорожніх знаків, облич, визначення ризику захворювання та багато іншого.
	А з поширенням його використання, також збільшуєтья і ризик нападів зловмисників на ці алгоритми, що може привести, до трагічних наслідків. Тому варто досліджувати тему нападів на алгоритми машинного навчання, та знати як захистити свою модель. \par
	В межах теми цієї роботи будуть розглядатись різні атаки на лінійні моделі машинного начання, та методи їх захисту.

	
	\section{Постановка задачі} 
	
	\textit{Мета} даної роботи полягає у тому, щоб дослідити ефективність атак на лінійні моделі машинного навчання, та визначити методи захисту від них. \par
	Виходячи з мети, визначені завдання роботи:
	\begin{itemize}
	\item Практична реалізація та дослідження методів атак
	\item Визначення методів захисту
	\end{itemize}

	
	\section{Класифікація атак}
		
	Нехай існує класифікатор $f(x):x\rightarrow y$, де  $x \in X, y \in Y$, який передбачає значення $y$ для вхідного $x$. Метою змагального прикладу є знайти такий $x^{*}$, який знаходиться в околі $x$, але хибно класифікується класифікатором. Зазвичай максимальний рівень шуму який може бути в змагальному прикладі може бути не більше за певну $L_p$ норму $ \| x^{*} - x \|_p < \epsilon $, де $p=1,2,\infty $.
	\newline \par
	\textbf{Нецілеспрямовані атаки (Untargeted Attacks)} \newline Нецілеспрямованими атаками є атаки метою яких є внести в приклад $x$, який правильно передбачається як $f(x) = y$ незначний шум так, щоб класифікатор зробив хибне передбачення $f(x^{*}) \neq y$.
	\newline \par
	\textbf{Цілеспрямовані атаки (Targeted Attacks)} \newline
	Цілеспрямованими атаками називають такі атаки, метою яких є внести у вхідний приклад $x$ такі пертурбації щоб класифікатор передбачив якийсь конкретний клас $f(x^{*}) = y^{*}$, де $y^{*}$ є заданою ціллю $y^{*} \neq y$.
	\newline \par
	\textbf{Атаки на закриту модель (White Box attacks)} - описує такий сценарій коли в нападника є повний доступ до параметрів і градієнтів моделі.  
	\newline \par
	\textbf{Атаки на відкриту модель (Black Box attacks)} - це атаки при яких в нападника нема доступу до параметрів моделі. 
	% ============================================ %

	% ============================================ %
	\chapter{Модель і датасет}
	
	\section{Мультикласова логістична регресія}
	В якості лінійного методу машинного навчання, на котрий будуть сдійснюватись атаки буде використовуватись модифікований алгоритм логістичної регресії для мультикласової класифікації. \par
	Нехай маємо набір тренувальних даних:
	\begin{equation*}
		(x^{(1)}, y^{(1)}), \quad (x^{(2)}, y^{(2)}), \quad ... \quad ,(x^{(m)}, y^{(m)})
	\end{equation*}
	\begin{equation*}
		x \in
		\begin{bmatrix}
		x_1   \\
		\dots \\
		x_n
		\end{bmatrix}	
		\quad \quad \quad
		y \in
		\begin{bmatrix}
		y_1   \\
		\dots \\
		y_C
		\end{bmatrix}
		\quad
		y_1, ..., y_C
	\end{equation*}
	\begin{equation*}
		X =	
		\begin{bmatrix}
		\vdots  & \vdots  & & \vdots \\
		x^{(1)} & x^{(2)} &   \dots & x^{(m)}\\
		\vdots  & \vdots  & & \vdots
		\end{bmatrix}
		\quad \quad \quad
		Y =	
		\begin{bmatrix}
		\vdots  & \vdots  & & \vdots \\
		y^{(1)} & y^{(2)} &   \dots & y^{(m)}\\
		\vdots  & \vdots  & & \vdots
		\end{bmatrix}
	\end{equation*}
	\newline
	$X$ - матриця в якої кожен стовпець є набором характеристик $i-$ого прикладу, $i=1,\dots,m$ 
	\newline
	$Y$ - матриця класів в якої кожен стовпець це масив розмірності $C$, з одиницею на місті справжнього класу
	\newline
	$m$ - кількість характеристик
	\newline
	$n$ - кількість характеристик в кожному прикладі
	\newline
	$C$ - кількість класів
	\newline \newline
	Задача прикладу $x \in R^{n}$, знайти $\hat{y}=P(y = 1 \mid x), \quad 0 \leq \hat{y} \leq 1$
	\begin{center}
	%Шукатимемо у вигляді:
	$\hat{y} = softmax(\omega\tran x + b)$, \quad де $\omega \in R^{n}, b \in R$ - невідомі параметри
	\end{center}
	Функція активації матиме вигляд:
	\begin{equation}
		softmax(z) = \frac{e^{z}}{\sum_{k=1}^{c} e_k^z}
	\end{equation}
	Для кожного прикладу з тренувального датасету потрібно обчислити:
	\begin{center}
	$ z\ith = \omega\tran x\ith + b $, \quad де	$y\ith = softmax(z_i) = \frac{e^{z_i}}{\sum_{k=1}^{c} e_k^z}$, так щоб $\hat{y}\ith \approx y\ith $
	\end{center}
	В якості функції втрати буде використовуватись функція кросс-ентропії:
	\begin{equation}
		\xi(y, x) = - \sum_{i=1}^{c} y_i  \log (\hat{y_i})
	\end{equation}
	Задача полягає в тому щоб знайти параметри $w \in R^n_x, b\in R$ що мінімізують функцію $J(\omega, b)$
	Для цього будемо використовувати алгоритм градієнтного спуску
	
	\section{Датасет}
	Для аналізу атак на лінійні методи машинного навчання буде використовуватись MNIST база даних яка складаэться з рукописних цифр. На її основі буде здійснюватись тренування моделі та аналіз атак.
	% ============================================ %
	
	% ============================================ %
	\chapter{Методи атак} 

	\section{Атака Шумом}

	\section{Fast Gradient Sign методи}
	Першим методом атаки який буде розглядатись є методу швидкого градієнту (Fast Gradient Sign Method).

	\subsection{FGSM}
	Першим методом атаки який буде розглядатись є методу швидкого градієнту (Fast Gradient Sign Method).
	
	Ідея цього методу полягає в тому, щоб знайти такий adversarial приклад $x^{*}$ який максимізую функцію втрати $J(x^{*}, y)$, де J є функцією кросс-ентропії.
	$$ X^{*} = X + \epsilon \cdot sign(\Delta_x J(x, y))$$
	$$\quad \Delta_x J(x, y)$$	
	
	\textbf{Обчислення градієнту}
	\newline
	Для методу логістичної регресії необхідно обчислити градієнт від функції кросс-ентропії(посилання).
	%$$z_i = \omega\tran x\ith + b \hat{y}_i = a_i = softmax(z_i)$$
	\begin{align*}
		z_i = \omega\tran x\ith + b \quad
		\hat{y}_i = a_i = softmax(z_i) \quad
		softmax(z_i) = \frac{e^{z_i}}{\sum_{k=1}^{c} e_k^z} \quad
	\end{align*}
	\textbf{Похідна softmax функції} \newline
	Якщо $i = j$,
	\begin{align*}
	    \pdv{\hat{y_i}}{z_j} 
	    &=
	    \pdv{\frac{e^{z_i}}{\sum_{k=1}^{c} e_k^z}}{z_j} 
	    =
	    \frac{e^{z_i} \sum_{k=1}^{c} e^{z_k} - e^{z_j} e^{z_i}}{(\sum_{k=1}^{c}  e^{z_k})^2} 
	    = \\& =
	    \frac{e^{z_j}}{\sum_{k=1}^{c}  e^{z_k}} \times \frac{(\sum_{k=1}^{c} e^{z_k} - e^{z_j})}{\sum_{k=1}^{c}  e^{z_k}} 
	    = 
	    y_i(1-y_j)
	\end{align*}
	Якщо $i \neq j$,
	\begin{align*}
		\pdv{\hat{y_i}}{z_j}
		&=
		\pdv{\frac{e^{z_i}}{\sum_{k=1}^{c} e_k^z}}{z_j} 
		=
		\frac{0 - e^{z_j} e^{z_i}}{\sum_{k=1}^{c}  e^{z_k}} \times \frac{e^{z_i}}{\sum_{k=1}^{c} e^{z_k}} 
		= 
		-y_i y_j 
	\end{align*}
	
	\textbf{Функція кросс-ентропії}
	\begin{align*}
		\xi(y, x) = - \sum_{i=1}^{c} y_i  \log (\hat{y_i})
	\end{align*}
	
	\textbf{Функція кросс-ентропії}
	\begin{align*}
		\pdv{\xi}{z_i} 
		&= 
		- \sum_{j=1}^{c} \pdv{ y_j \log (\hat{y_j})}{z_i} 
		=
		- \sum_{j=1}^{c} y_j \pdv{ \log (\hat{y_j})}{z_i} 
		= 
		- \sum_{j=1}^{c} y_j \frac{1}{\hat{y_i}} \cdot \pdv{\hat{y_i}}{z_i} 
		= \\& =
		- \frac{y_i}{\hat{y_i}} \pdv{\hat{y_i}}{z_i} - \sum_{j \neq i} \frac{y_j}{\hat{y_j}} \pdv{\hat{y_j}}{z_i} 
		= 
		-\frac{y_i}{\hat{y_i}} \hat{y_i} (1 - \hat{y_i}) - \sum_{j \neq i} \frac{y_j}{\hat{y_j}} (-\hat{y_i} \hat{y_j}) 
		= \\& =
		-y_i + y_i \hat{y_i} + \sum_{j \neq i} y_j \hat{y_i}
		= 
		-y_i + \sum_{j=1}^{c} y_j \hat{y_i} 
		=
		\hat{y_i} - y_i
	\end{align*}
	, i = 1, ..., C
	
	\textbf{Обчислення урадієнту "має бути вище"}
	\begin{align*}
		\pdv{\xi}{x_i} 
		&=
		\pdv{\xi}{z_i} \pdv{z_i}{x_i} 
		=
		(\hat{y_i} - y_i) \pdv{(\omega\tran x_i+ b)}{x_i} 
		=
		(\hat{y_i} - y_i) x_i 
		=
		dz \cdot x
	\end{align*}
	
	\textbf{Обчислення урадієнту "має бути вище"}
	
	%\begin{algorithmic}[1]	
	%	\While{Termination condition not reached}
	%	\If {$N$ is even}
	%	\State $X \Leftarrow X \times X$
	%	\State $N \Leftarrow N / 2$
	%	\Else {$N$ is odd}
	%	\State $y \Leftarrow y \times X$
	%	\State $N \Leftarrow N - 1$
	%	\EndIf
	%	\State return ...
	%	\EndWhile
	%\end{algorithmic}

	\subsection{T-FGSM}

	\subsection{I-FGSM}
		\begin{algorithm}
			\caption{$I-FGSM$}
			\begin{algorithmic}[1]
				\State \algorithmicrequire{Приклад $x$, значення цілі y, класифікатор $f$}
				\State \algorithmicrequire{значення пертурбації $\epsilon$, значення цілі y, кількість ітерації $T$.}
				\State \algorithmicensure{ Adversarial $x^{*}$ з нормою $\|x^{*} - x\|_{inf} \leq \epsilon $;}
				\State $a = \epsilon / T $;
				\State $x^{*} = x$;
				\For{$t=0 \quad to \quad T-1$}
				\State $x^{*} = x^{*} + \alpha \cdot sign(\Delta_x J(x, y))$;
				\EndFor
				\State \algorithmreturn{$x^{*} = x^{*}_{T}$}.
			\end{algorithmic}
		\end{algorithm}
		

	\subsection{MI-FGSM}
	\begin{algorithm}
	\caption{$MI-FGSM$}
		\begin{algorithmic}[1]
			\State \algorithmicrequire{Приклад $x$, значення цілі y, класифікатор $f$.};
			\State \algorithmicrequire{значення пертурбації $\epsilon$, значення цілі y, кількість ітерації $T$.};
			\State \algorithmicensure{ Adversarial $x^{*}$ з нормою $\|x^{*} - x\|_{inf} \leq \epsilon $};
			\State $a = \epsilon / T$;
			\State $g0 = 0;$ $x^{*}_0 = x$;
			\For{$t=0 \quad to \quad T-1$}
			\State $g_{t+1} = \mu \cdot g_t + \frac{\Delta_x J(x^{*}, y)}{\|\Delta_x J(x^{*}, y)\|_1}$;
			\State $x^{*}_{t+1} = x^{*}_{t} + \alpha \cdot sign(g_{t+1})$;
			\EndFor
			\State \algorithmreturn{$x^{*} = x^{*}_{T}$}.
		\end{algorithmic}
	\end{algorithm}
	
	\section{DeepFool}
	\begin{algorithm}
		\caption{$DeepFool$}
		\begin{algorithmic}[1]
			\State \algorithmicrequire{Приклад $x$, значення цілі y, класифікатор $f$.};
			\State \algorithmicrequire{значення цілі y, кількість ітерації $T$.};
			\State \algorithmicensure{ Adversarial $x^{*}$ з нормою $\|x^{*} - x\|_{inf} \leq \epsilon $};
			\State $x_0 = x$; $i = 0$; 
			\While{$\hat{f}(x_i) = \hat{f}(x_0) $}
				\For{$k \neq \hat{f}(x_0)$}
					\State $ w_{k}' = \Delta f_k(x_i) - \Delta f_{\hat{k}(x_0)} (x_i)$;
					\State $ f_{k}' = \Delta f_k(x_i) - \Delta f_{\hat{k}(x_0)} (x_i)$;
				\EndFor
				\State $\hat{l} = \arg \min_{k \neq \hat{f}(x_0)} \frac{\abs{f_{k}'}}{\|w_{k}'\|_2}$;
				\State $r_i = \frac{\abs{f_{\hat{l}}'}}{\|w_{\hat{l}}'\|_{2}^{2}} w_{\hat{l}}'$;
				\State $x_{i+1} = x_i + r_i$;
				\State $i = i + 1$;
			\EndWhile
			\State \algorithmreturn{$x^{*} = x^{*}_{T}$}.
		\end{algorithmic}
	\end{algorithm}
	
	\section{Нецілеспрямовані атаки} 
	Основна частина даної роботи полягала у написанні програми. Нижче наводимо основний алгоритм її роботи, на мові C:
	
	\begin{lstlisting}[language=Python]
	#include &lt;stdio.h&gt;
	int main() 
	{ 
	printf("Hello, world!\n"); 
	return 0; 
	} 
	\end{lstlisting}[language=Python]

	\lstset{language=Python}
	\begin{lstlisting}
	def fit(self, X, w, b, y, alpha, max_iters, predict_func):
		# Check that X and y have correct shape
		self.w = w
		self.b = b
		
		self.y_ = np.expand_dims(y.T, axis=1)
		self.X_ = X.T
		
		self.num_iters = 0
		self.X_ = self._gradient_descent(self.X_, self.y_, self.w, self.b, alpha, max_iters, predict_func)
		
		def _cost_function(self, X, Y, A):
		m = X.shape[0]
		if m == 0:
		return None
		
		J = (1 / m) * np.sum(-Y * np.log(A) - (1 - Y) * np.log(1 - A))
		return J
	\end{lstlisting}
	
	%\begin{algorithm}[H]
	%	\SetAlgoLined
	%	\KwResult{Write here the result }
	%	initialization\;
	%	\While{While condition}{
	%		instructions\;
	%		\eIf{condition}{
	%			instructions1\;
	%			instructions2\;
	%		}{
	%			instructions3\;
	%		}
	%	}
	%	\caption{How to write algorithms}
	%\end{algorithm}
	% ============================================ %	
	
	% ============================================ %
	\chapter{Альтернативні рішення} 
	Деякі дослідники пишуть свої роботи в програмах типу Microsoft Word. Але то не є труйово\cite{howto}.	
	
	% ============================================ %	
	\chapter{Висновок} 
	Дана робота містить значний мій вклад, і перевершує попередні досягнення в багатьох напрямках. Окрім того, даний напрямок досліджень має значні перспективи
	подальшого розвитку. (Особливо добре було б, якби хтось вирішив проблему кирилиці в listings).
	% ============================================ %
		
	% ============================================ %	
	\newpage
	\addcontentsline{toc}{chapter}{Література}
	\begin{thebibliography}{9}
		
		\bibitem{howto} Вікіпідручник \emph{Як написати курсову?}
		(\url{http://uk.wikibooks.org/wiki/%D0%AF%D0%BA_%D0%B2%D1%87%D0%B8%D1%82%D0%B8%D1%81%D1%8C_%D0%BA%D1%80%D0%B0%D1%89%D0%B5%3F/%D0%9A%D1%83%D1%80%D1%81%D0%BE%D0%B2%D1%96})
			
		\end{thebibliography}
	% ============================================ %
	
\end{document}