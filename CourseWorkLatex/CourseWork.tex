% -*- mode: LaTeX; coding: utf8; -*-

\documentclass[a4paper,14pt]{extreport}

\usepackage[utf8]{inputenc}
\usepackage[english,ukrainian]{babel}
\usepackage[T1, T2A]{fontenc}
\usepackage{dsfont}
\usepackage[export]{adjustbox}

\usepackage{indentfirst}
\selectlanguage{ukrainian}

\usepackage{fontspec}
\setmainfont{Times New Roman}

% інтервал - півтора. 
\usepackage{setspace}
\onehalfspacing

% поля
\usepackage{geometry}
\geometry{a4paper}
\geometry{left=25mm,right=10mm,top=20mm,bottom=20mm}
\geometry{headheight=2ex,headsep=10mm,footskip=10mm}

%
\usepackage{mathtools}
\usepackage{mathrsfs, amsmath}

%для картинок
\usepackage{graphicx}
\usepackage{caption}

% lstlisting settings
\usepackage{xcolor}

% для часткових похідних
\usepackage{physics}

\usepackage{color}
\usepackage{listings}

\usepackage{algorithmicx}

\lstset{
	extendedchars=\true, % дозволити кирилицю в лінстингу
	inputencoding=utf8,
	breaklines=true,
	basicstyle=\ttfamily,
	numbers=left,
	frame=single,
	backgroundcolor=\color{bluegray}
}

% якщо прийдеться вставляти код (простіше ніж listings, але немає breaklines)
\usepackage{verbatim}

% для алгоритмів
%\usepackage[ruled,vlined]{algorithm2e}
\usepackage{algpseudocode}
\usepackage{algorithm}
\floatname{algorithm}{Algorithm}
\renewcommand{\algorithmicrequire}{\textbf{Input: }}
\renewcommand{\algorithmicensure}{\textbf{Output: }}
\newcommand{\algorithmreturn}{\textbf{return }}

% bibliography
\usepackage[
backend=biber,
style=numeric,
sorting=ynt
]{biblatex}
\addbibresource{resources/bibres.bib}

% custom commands
\newcommand{\tran}{^{T}}
\newcommand{\ith}{^{(i)}}

\begin{document}
	% ============================================ %
	\begin{titlepage}%
    	\begin{center}
	    	{\textbf{ЛЬВІВСЬКИЙ НАЦІОНАЛЬНИЙ УНІВЕРСИТЕТ \\ ІМЕНІ ІВАНА ФРАНКА}}\par
	       	{Факультет прикладної математики та інформатики \\ Кафедра обчислювальної математики}\par
			\begin{center}
	
	        \end{center}
	        \vspace{25mm}
	        {\textbf{\huge{Курcова робота}}}\par
	        \vspace{5mm}
	        {АНАЛІЗ АТАК НА ЛІНІЙНІ МОДЕЛІ МАШИННОГО НАВЧАННЯ}\par
	        \vspace{5mm}
	        {}\par %subtitle
        \end{center}
	   	
	   	\vspace{30mm}
	   	
		\begin{flushright}
   	   		\begin{minipage}[t]{110mm}
   	   			\flushleft{
   	   			{\large{
			   	   		Виконав: студент \underline{III} курсу групи \underline{ПМп-31} напрямку підготовки (спеціальності) \\
			   	   		113 $-$ "Прикладна математика" \hfill \\
				}}
	   	   	   	{\large{
	   	   	   			\noindent\underline{\makebox[110mm]{\hfill Середович В.В.}} \\
   	   	   		}}
      			{\small{
    					\hfill \footnotesize{(прізвище та ініціали)} \\
	   	   		}}
	   	   		{\large{
	   	   				Керівник: \noindent\underline{\makebox[83.3mm]{\hfill Музичук Ю.А.}}
	   	   		}}
      			} \\
	   	   		{\small{
			   	   		\hfill \footnotesize{(прізвище та ініціали)} \\
			   	   		\vspace{2ex}
				}}
   	   		\end{minipage}
   	   \end{flushright}
	   \vfill
   	   
   	   \begin{center}Львів --- 2020\end{center}
   	   \stepcounter{page}
    \end{titlepage}
	% ============================================ %
	\addtocontents{toc}{\protect\thispagestyle{empty}}
	\tableofcontents
	% ============================================ %
	\addcontentsline{toc}{section}{Вступ}
	\chapter*{Вступ}
	Алгоритми машинного навчання активно використовується у різних областях нашого життя та дуже добре демонструють себе у задачах класифікації. Однак, виявляється, що ці алгоритми є досить вразливими навіть до незначних, правильно підібраних, пертурбацій вхідної інформації. Такі модифіковані приклади, які навмисно змінють для того, щоб вони були хибно передбаченні називають змагальними прикладами або adversarial samples. В деяких випадках ці зміни можуть бути настільки малі, що для людини вони зовсім не будуть помітні, однак класифікатор моделі буде робити хибне передбачення. Така вразливість може бути дуже небезпечною, коли алгоритми машинного навчання застосовуються в критичних для здоров'я людини середовищах. Саме тому ця область привертає до себе багато уваги і потребує досліджень. 
	
	В межах теми цієї роботи будуть розглядатись різні методи атак на лінійні моделі машинного навчання які будемо аналізувати і порівнювати між собою. Також будуть запропоновані методи захисту моделі.
	% ============================================ %

	\chapter{Постановка задачі} 
	\textit{Мета} даної роботи полягає у тому, щоб дослідити ефективність атак на лінійні моделі машинного навчання, та визначити методи захисту від них. \par
	Виходячи з мети, визначені завдання роботи:
	\begin{itemize}
	\item Реалізувати лінійну модель машинного навчання
	\item Дослідити різні методи генерування змагальних прикладів
	\item Проаналізувати ефективнівсть атак
	\item Визначити методи захисту
	\end{itemize}

	
	\section{Основні поняття}
	
	В цій секції будуть розглянуті основні поняття, які будуть використовуватись в ході роботи. 
	
	\textbf{Змагальний приклад (Adversarial sample)}
	\newline
	Нехай існує класифікатор $f(x):x\rightarrow y$, де  $x \in X, y \in Y$, який передбачає значення $y$ для вхідного $x$. Метою змагального прикладу є знайти такий $x^{*}$, який знаходиться в околі $x$, але хибно визначається класифікатором. Зазвичай максимальний рівень шуму який може бути в змагальному прикладі може бути не більше за певну $L_p$ норму $ \| x^{*} - x \|_p < \varepsilon $, де $p=1,2,\infty $. В межах даної роботи для визначення рівня пертурбації буде використовуватись саме $L_{\infty}$ норма.
	\par
	\textbf{Націлені атаки (Targeted Attacks)} \newline
	Націлені атаками є атаки метою яких є внести в приклад $x$, який правильно передбачається як $f(x) = y$ незначний шум так, щоб класифікатор зробив хибне передбачення $f(x^{*}) \neq y$.
	\par
	\textbf{Ненацілені атаки (Untargeted Attacks)} \newline
	Ненаціленими атаками називають такі атаки, метою яких є внести у вхідний приклад $x$ такі пертурбації щоб класифікатор передбачив якийсь конкретний клас $f(x^{*}) = y^{*}$, де $y^{*}$ є заданою ціллю $y^{*} \neq y$.
	\par
	\textbf{Атаки на відкриту модель (White Box attacks)} \newline
	До атак на відкриту модель відносять атаки, при яких в нападника нема доступу до параметрів моделі. 
	\par
	\textbf{Атаки на закриту модель (Black Box attacks)} \newline
	Атаки на закриту модель описують такий сценарій, коли в нападника є повний доступ до параметрів і градієнтів моделі.
	% ============================================ %

	% ============================================ %
	\chapter{Модель і датасет}
	
	\section{Мультикласова логістична регресія}
	\label{sec:lg}
	В якості лінійного методу машинного навчання, на котрий будемо здійснювати атаки буде використовуватись модифікований алгоритм логістичної регресії для мультикласової класифікації. \par
	Нехай маємо набір тренувальних даних:
	\begin{equation*}
		(x^{(1)}, y^{(1)}), \quad (x^{(2)}, y^{(2)}), \quad ... \quad ,(x^{(m)}, y^{(m)})
	\end{equation*}
	\begin{equation*}
		\quad
		%%%
		X =	
		\begin{bmatrix}
		\vdots  & \vdots  & & \vdots \\
		x^{(1)} & x^{(2)} &   \dots & x^{(m)}\\
		\vdots  & \vdots  & & \vdots
		\end{bmatrix}
		%%%
		\quad
		%%%
		x \in
		\begin{bmatrix}
		x_1   \\
		\dots \\
		x_n
		\end{bmatrix}
		\hspace{21 mm}
		%%%
	\end{equation*}
	\begin{equation*}
		\quad
		%%%
		Y =	
		\begin{bmatrix}
		\vdots  & \vdots  & & \vdots \\
		y^{(1)} & y^{(2)} &   \dots & y^{(m)}\\
		\vdots  & \vdots  & & \vdots
		\end{bmatrix}
		%%%
		\quad
		%%%
		y \in
		\begin{bmatrix}
		y_1   \\
		\dots \\
		y_C
		\end{bmatrix}
		\quad
		y_1, ..., y_C
		%%%
	\end{equation*}
	\newline
	$X$ - матриця в якої кожен стовпець є набором характеристик $i-$ого прикладу, \newline $i=1,\dots,m$ 
	\newline
	$Y$ - матриця класів в якої кожен стовпець це масив розмірності $C$, з одиницею на місті справжнього класу
	\newline
	$m$ - кількість характеристик
	\newline
	$n$ - кількість характеристик в кожному прикладі
	\newline
	$C$ - кількість класів
	\newline \newline
	Задача прикладу $x \in R^{n}$, знайти $\hat{y}=P(y = 1 \mid x), \quad 0 \leq \hat{y} \leq 1$
	\begin{center}
	%Шукатимемо у вигляді:
	$\hat{y} = softmax(\omega\tran x + b)$, \quad де $\omega \in R^{n}, b \in R$ - невідомі параметри
	\end{center}
	Функція активації матиме вигляд:
	\begin{equation}	
		softmax(z) = \frac{e^{z}}{\sum_{k=1}^{C} e_k^z}
	\end{equation}
	Для кожного прикладу з тренувального датасету потрібно обчислити:
	\begin{equation*}
	z\ith = \omega\tran x\ith + b, \; \textup{де} \; \hat{y}\ith = softmax(z_i) = \frac{e^{z_i}}{\sum_{k=1}^{C} e_k^z}, \; \textup{так щоб} \; \hat{y}\ith \approx y\ith
	\end{equation*}
	В якості функції втрати буде використовуватись функція кросс-ентропії:
	\begin{align}
		\label{eq:cross-entropy}
		&\xi(y\ith, x\ith) = -\sum_{j=1}^{C} y_j \log(\hat{y_j}) \\
		&\xi(Y, X) =\frac{1}{m} \sum_{i=1}^{m}\xi(y\ith, x\ith) = -\frac{1}{m} \sum_{i=1}^{m} \sum_{j=1}^{C} y_{j}\ith \log (\hat{y}_j\ith) = J(\omega, b)
	\end{align}
	Задача полягає в тому щоб знайти параметри $w \in R^n_x, b\in R$ що мінімізують \newline функцію $J(\omega, b)$.
	Для цього будемо використовувати алгоритм градієнтного спуску.
	
	\section{Датасет}

	Для аналізу атак на лінійні методи машинного навчання буде використовуватись MNIST база даних яка складається з 60 тис. тренувальних зображень та 10 тис. тестувальних рукописних цифр.
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.4 \textwidth]{resources/minist_dataset.jpg}
		\caption{датасет рукописних цифр MNIST}
		\label{fig:minist_dataset}
	\end{figure}
	На її основі буде здійснюватись тренування моделі та аналіз атак.
	
	\section{Тренування моделі}
	
	
	
	
	\label{sec:model}
	% ============================================ %
	
	% ============================================ %
	\chapter{Методи атак}
	В цьому розділі будуть розглянуті деякі методи генерування змагальних зразків, які були реалізовані і протестовані на лінійній моделі натренованій в розділі \ref{sec:model}.
	
	Далі, під час опису будуть використовуватись такі позначення:
	\begin{itemize}
		\item $\boldsymbol{X}$ - це зображення, зазвичай це 3-D тензор (ширина $\times$ висота $\times$ глибина). В нашому випадку має розміри $(28 \times 28 \times 1)$ пікселей, яке складаеться з додатніх цілих чисел в межах $[0, 255]$.
		\item $\boldsymbol{X}^{*}$ - згенерований змагальний приклад для зображення $\boldsymbol{X}$
		\item $y_{true}$ - правильний клас для зображення $\boldsymbol{X}$.
		\item $J(\boldsymbol{X}, y)$ - штрафна функція моделі. Ваги моделі $\omega$ навмисно не будемо вказувати, вважатимемо їх сталими, як значення які будуть отримані з моделі машинного навчання. В нашому випадку штрафною функцією є функція кросс-ентропії \ref{eq:cross-entropy}. 
		\item $Clip_{\boldsymbol{X}, \varepsilon} \{ \boldsymbol{X}^{*} \}$ - функція яка виконує по-піксельне обрізання зображення $X^{*}$ так, щоб результат був в $L_{\infty}$ $\varepsilon $ - околі вхідного зображення $\boldsymbol{X}$. Рівняння для цього виглядає наступним чином:
		
		\begingroup
		\setlength{\abovedisplayskip}{0pt}
		\setlength{\belowdisplayskip}{0pt}
		\begin{align}
		\label{eq:clip}
		\resizebox{.9 \textwidth}{!} 
		{
			$
			Clip_{X, \varepsilon} \{ \boldsymbol{X}^{*} \}(x, y, z) = 
			min\Big\{ 255, \boldsymbol{X}(x, y, z) + \varepsilon, max \{ 0, \boldsymbol{X}(x, y, z) - \varepsilon, \boldsymbol{X}^{*}(x, y, z) \} \Big\}
			$,
		}
		\end{align}
		\endgroup

		де $\boldsymbol{X}(x, y, z)$ - це значення каналу $z$ зображення $\boldsymbol{X}$ з координатами $(x, y)$.
	\end{itemize}

	\section{FGSM}
	Одним з найпошириніших методів для генерування змагальних зразків машинного навчання є Fast Gradient Sign Method. В перше він був представлений в роботі \textcite{goodfellow2014explaining}. Ідея цього методу полягає в тому, щоб знайти такий змагальний приклад $x^{*}$ який максимізує функцію втрати $J(x^{*}, y)$ до певного $L_{\infty}$ обмеження. Цього можна досягти один раз використавши зворотне поширення:
	\begin{equation}
	X^{*} = X + \varepsilon \cdot sign(\Delta_x J(x, y)),
	\end{equation}
	де $\varepsilon$ - деякий гіперпараметер.
	
	Для того щоб застосувати цей метод для логістичної регресії необхідно обчислити градієнт від функції кросс-ентропії \ref{eq:cross-entropy}. Для цього спочатку знайдемо похідну від $softmax$ функції:
	\begin{align*}
		z_i = \omega\tran x\ith + b; \quad
		\hat{y}_i = a_i = softmax(z_i); \quad
		softmax(z_i) = \frac{e^{z_i}}{\sum_{k=1}^{C} e_k^z}; \quad
	\end{align*}
	Якщо $i = j$,
	\begin{align}
	    \pdv{\hat{y_i}}{z_j} 
	    &=
	    \pdv{\frac{e^{z_i}}{\sum_{k=1}^{C} e_k^z}}{z_j} 
	    =
	    \frac{e^{z_i} \sum_{k=1}^{C} e^{z_k} - e^{z_j} e^{z_i}}{(\sum_{k=1}^{C}  e^{z_k})^2} 
	    = \\& =
	    \frac{e^{z_j}}{\sum_{k=1}^{C}  e^{z_k}} \times \frac{(\sum_{k=1}^{C} e^{z_k} - e^{z_j})}{\sum_{k=1}^{C}  e^{z_k}} 
	    = 
	    y_i(1-y_j)
	\end{align}
	Якщо $i \neq j$,
	\begin{align}
		\pdv{\hat{y_i}}{z_j}
		&=
		\pdv{\frac{e^{z_i}}{\sum_{k=1}^{C} e_k^z}}{z_j} 
		=
		\frac{0 - e^{z_j} e^{z_i}}{\sum_{k=1}^{C}  e^{z_k}} \times \frac{e^{z_i}}{\sum_{k=1}^{C} e^{z_k}} 
		= 
		-y_i y_j 
	\end{align}
	
	\begingroup
	\setlength{\abovedisplayskip}{0pt}
	\setlength{\belowdisplayskip}{0pt}
	
	Далі обчислюємо похідну від функції кросс-ентропії:
	\begin{align*}
		\xi(y, x) = - \sum_{i=1}^{C} y_j  \log (\hat{y_j})
	\end{align*}

	\begin{align*}
		\pdv{\xi}{z_i} 
		&= 
		- \sum_{j=1}^{C} \pdv{ y_j \log (\hat{y_j})}{z_i} 
		=
		- \sum_{j=1}^{C} y_j \pdv{ \log (\hat{y_j})}{z_i} 
		= 
		- \sum_{j=1}^{C} y_j \frac{1}{\hat{y_i}} \cdot \pdv{\hat{y_i}}{z_i} 
		= \\& =
		- \frac{y_i}{\hat{y_i}} \pdv{\hat{y_i}}{z_i} - \sum_{j \neq i} \frac{y_j}{\hat{y_j}} \pdv{\hat{y_j}}{z_i} 
		= 
		-\frac{y_i}{\hat{y_i}} \hat{y_i} (1 - \hat{y_i}) - \sum_{j \neq i} \frac{y_j}{\hat{y_j}} (-\hat{y_i} \hat{y_j}) 
		= \\& =
		-y_i + y_i \hat{y_i} + \sum_{j \neq i} y_j \hat{y_i}
		= 
		-y_i + \sum_{j=1}^{C} y_j \hat{y_i} 
		=
		\hat{y_i} - y_i ,
	\end{align*}
	\endgroup
	де i = 1, ..., C
	\newline	
	Тепер можна обчислити градієнт від штрафної функції по $x$:
	\begin{align}
		\pdv{\xi}{x_i} 
		&=
		\pdv{\xi}{z_i} \pdv{z_i}{x_i} 
		=
		(\hat{y_i} - y_i) \pdv{(\omega\tran x_i+ b)}{x_i} 
		=
		(\hat{y_i} - y_i) \omega\tran 
		=
		dz \cdot \omega\tran
	\end{align}
	 Маючи всі наведені обчислення, можна записати алгоритм знаходження змагального прикладу [\ref{alg:fgsm}].
	\begin{algorithm}
		\caption{$FGSM$}
		\label{alg:fgsm}
		\begin{algorithmic}[1]
			\State \algorithmicrequire{Приклад $x$, клас $y_{true}$, класифікатор $f$, параметри: $\omega, b$.}
			\State \algorithmicrequire{значення пертурбації $\varepsilon$.}
			\State \algorithmicensure{ Adversarial $x^{*}$ з нормою $\|x^{*} - x\|_{\infty} \leq \varepsilon $;}
			\State $z = \omega\tran x + b$;
			\State $\hat{y} = softmax(z)$;
			\State $\delta = (\hat{y} - y_{true}) \cdot \omega\tran$;
			\State $x^{*} = Clip_{X, \varepsilon} \big\{ x + \alpha \cdot sign( \delta ) \big\}$;
			\State \algorithmreturn{$x^{*} = x^{*}_{T}$}.
		\end{algorithmic}
	\end{algorithm}
	
	\section{I-FGSM}
	Даний метод є модифікацією попереднього, особливість якого полягає в тому, що для знахождення змагального прикладу буде використовуватись ітераційний підхід.	Ми застосовуємо звичайний метод декілька разів з певним невеликим кроком $a$ і після кожного кроку використовуємо функцію $clip$ \ref{eq:clip}, для того щоб переконатись що значення знаходяться в $\varepsilon$-околі оригінального зображення.

	Ітераційний алгоритм швидкого градієнтного спуску буде мати вигляд [\ref{alg:i-fgsm}].
	
	\begin{algorithm}
		\caption{$I-FGSM$}
		\label{alg:i-fgsm}
		\begin{algorithmic}[1]
			\State \algorithmicrequire{Приклад $x$, значення класу $y_{true}$, класифікатор $f$}
			\State \algorithmicrequire{значення пертурбації $\varepsilon$, кількість ітерації $T$.}
			\State \algorithmicensure{ Adversarial $x^{*}$ з нормою $\|x^{*} - x\|_{\infty} \leq \varepsilon $;}
			\State $a = \varepsilon / T $;
			\State $x^{*} = x$;
			\For{$t=0 \; to \; T-1$}
			\State $x^{*} = Clip_{X, \varepsilon} \big\{ x^{*} + \alpha \cdot  sign(\Delta_x J(x, y)) \big\}$;
			\EndFor
			\State \algorithmreturn{$x^{*} = x^{*}_{T}$}.
		\end{algorithmic}
	\end{algorithm}
		
		
	\begin{figure}[h]
		\centering
		\includegraphics[width=\textwidth]{resources/i-fgsm-example.png}
		\caption{}
		\label{fig:i-fgsm-example}
	\end{figure}
		
	\newpage
	\section{TI-FGSM}
	TI-FGSM або як його ще часто називаюсь Projected Gradient Descent є націленою версією попереднього алгоритму, а отже він дає можливість обрати клас який має передбачити модель. Два попередні методи описані вище намагалися максимізувати функцію втрати для правильного класу, не вказуючи яким буде неправильний класс. Цей метод - навпаки, мінімізує функцію втрати для деякого заданого неправильного класу. На сам алгоритм це вплине так, що на етапі додавання пертурбації у зображення знак буде змінений на протилежний.

	\begin{algorithm}
		\caption{$TI-FGSM$}
		\label{alg:ti-fgsm}
		\begin{algorithmic}[1]
			\State \algorithmicrequire{Приклад $x$, значення класу $y_{true}$, класифікатор $f$}
			\State \algorithmicrequire{значення пертурбації $\varepsilon$, кількість ітерації $T$.}
			\State \algorithmicensure{ Adversarial $x^{*}$ з нормою $\|x^{*} - x\|_{\infty} \leq \varepsilon $;}
			\State $a = \varepsilon / T $;
			\State $x^{*} = x$;
			\For{$t=0 \; to \; T-1$}
			\State $x^{*} = Clip_{X, \varepsilon} \big\{ x^{*} - \alpha \cdot  sign(\Delta_x J(x, y)) \big\}$;
			\EndFor
			\State \algorithmreturn{$x^{*} = x^{*}_{T}$}.
		\end{algorithmic}
	\end{algorithm}

	\section{MI-FGSM}	
	Попередні розглянуті методи хоч і є ефективними, але мають певні недоліки.
	Для прикладу однокроковий (FGSM), обчислює градієнт тільки один раз, припускаючи, що межа рішень є лінійною. На практиці це зазвичай не так і тому для змагального прикладу, згенерованого таким чином, характерне \textit{недотренування}, що обмежує ефективність атаки. З іншого боку ітеративний метод (I-FGSM), з кожною ітерацією жадібно рухає змагальний приклад в напрямку градієнту, що може призвести до зупинки в локальнії точці оптимума та \textit{перетренуванню} моделі. Щоб вирішити цю проблему в роботі \textcite{dong2017boosting} була представленна momentum оптимізація ітеративного методу. Вона допомагає стабілізувати напрямок зміни, прикладу що допомагає уникнути локального оптимума.
	\\{\textbf{TODO опис momentum метода}}  \\
	Алгоритм даної модифікації має наступний вигляд \ref{alg:mi-fgsm}	
	\begin{algorithm}
		\caption{$MI-FGSM$}
		\label{alg:mi-fgsm}
		\begin{algorithmic}[1]
			\State \algorithmicrequire{Приклад $x$, значення цілі y, класифікатор $f$.};
			\State \algorithmicrequire{значення пертурбації $\varepsilon$, значення цілі y, кількість ітерації $T$.};
			\State \algorithmicensure{ Adversarial $x^{*}$ з нормою $\|x^{*} - x\|_{inf} \leq \varepsilon $};
			\State $a = \varepsilon / T$;
			\State $g0 = 0;$ $x^{*}_0 = x$;
			\For{$t=0 \quad to \quad T-1$}
			\State $g_{t+1} = \mu \cdot g_t + \frac{\Delta_x J(x^{*}, y)}{\|\Delta_x J(x^{*}, y)\|_1}$;
			\State $x^{*} = Clip_{X, \varepsilon} \big\{ x^{*}_{t} + \alpha \cdot sign(g_{t+1}) \big\}$;
			\EndFor
			\State \algorithmreturn{$x^{*} = x^{*}_{T}$}.
		\end{algorithmic}
	\end{algorithm}
	
	\newpage
	\section{DeepFool}
	DeepFool була вперше представлена в роботі \textcite{moosavidezfooli2015deepfool}. На відмінно від FGSM методу цей алгоритм не можна використовувати для націлених атак, де є можливість вибрати клас який має передбачити модель. Отже, його можна використовувати тільки для генерування прикладів які будуть класифікуватись хибно. Основною метою цього методу є знаходження мінімального необхідного значення пертурбації, для того, щоб модель хибно передбачила клас прикладу.
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=\textwidth]{resources/deepfool.jpg}
		\caption{Приклад для лінійного бінарного класифікатора \cite{moosavidezfooli2015deepfool}}
		\label{fig:dfhyperline}
	\end{figure}

	Для випадку бінарної класифікації легко бачити, що надійність моделі $f$ в точці $ x_{0}, \; \Delta(x_{0}; f)$, дорівнює відстанні від $x_{0}$ до площини гіперпараметра $\mathscr{F} = \{ x: \omega\tran x + b = 0 \}$, яка розділяє два класи (Рис. \ref{fig:dfhyperline}). Таким чином, мінімальний зсув, необхідний для зміни рішення класифікатора відповідає ортогональній проекції $x_{0}$ на $\mathscr{F}$. Це можна записати у вигляді формули:
	\begin{align}
		r_{*}(x_{0}) := -\frac{f(x_{0})}{\norm{\omega}_{2}^{2}} \omega .
	\end{align}

	Для випалку $l_2$ норими DeepFool алгоритм для мультикласового випадку буде мати наступний вигляд [\ref{alg:deepfool}].

	\begin{algorithm}
		\caption{DeepFool: мультикласовий випадок}
		\label{alg:deepfool}
		\begin{algorithmic}[1]
			\State \algorithmicrequire{Приклад $x$, значення цілі y, класифікатор $f$.};
			\State \algorithmicrequire{значення цілі y, кількість ітерації $T$.};
			\State \algorithmicensure{ Adversarial $x^{*}$ з нормою $\|x^{*} - x\|_{2} \leq \varepsilon $};
			\State $x_0 = x$; $i = 0$; 
			\While{$\hat{f}(x_i) = \hat{f}(x_0) $}
				\For{$k \neq \hat{f}(x_0)$}
					\State $ w_{k}' = \Delta f_k(x_i) - \Delta f_{\hat{k}(x_0)} (x_i)$;
					\State $ f_{k}' = \Delta f_k(x_i) - \Delta f_{\hat{k}(x_0)} (x_i)$;
				\EndFor
				\State $\hat{l} = \arg \min_{k \neq \hat{f}(x_0)} \frac{\abs{f_{k}'}}{\|w_{k}'\|_2}$;
				\State $r_i = \frac{\abs{f_{\hat{l}}'}}{\|w_{\hat{l}}'\|_{2}^{2}} w_{\hat{l}}'$;
				\State $x_{i+1} = x_i + r_i$;
				\State $i = i + 1$;
			\EndWhile
			\State \algorithmreturn{$\hat{r} = \sum_{i} r_i $}.
		\end{algorithmic}
	\end{algorithm}
	

	Для того що реалізувати цей алгоритм необхідно спочатку знайти градієнт від функції кросс-ентропії \ref{eq:cross-entropy} по $x$.

	Маючи:
	\begin{equation}
		\pdv{\hat{y_i}}{x_k}  =
		\begin{cases}
		y_i(1-y_j) & \text{, якщо $i = j$}\\
		- y_i y_j & \text{, якщо $i \neq j$}\\
		\end{cases}
		\; \text{, де} \quad i = 1, .. ,C; \; j = 1, .. ,C;
	\end{equation}
	
	При знаходженні похідної нас цікавить випадок $i = j$, тому отримуємо:
	\begin{align*}
		\pdv{\hat{y_i}}{x_k} 
		&=
		\pdv{\hat{y_i}}{z_j} \pdv{z_j}{x_k} 
		=
		y_i(1-y_i) \pdv{(\omega\tran x_k + b)}{x_k} 
		=
		y_i(1-y_i) \omega\tran 
		=
		dz \cdot \omega\tran
	\end{align*}
		\begin{figure}[h]
		\centering
		\includegraphics[width=0.5\textwidth]{resources/deepfool-example.png}
		\caption{}
		\label{fig:deepfool-example}
	\end{figure}
		
	% ============================================ %	
	\chapter{Методи захисту}
	Маючи класифікатор моделі $F$ і деяке зображення $\hat{X}$, яке може бути або оригінальним зображенням $X$ або змагальним $X^{*}$, метою захисту моделі є або посилити $F$ як $F'$ таким чином, щоб $F(\hat{X}) = F(X)$, або трансформувати $\hat{X}$ через деякий метод $T$ так, щоб $F(T(\hat{X})) = F(X)$.
	
	
	\section{Randomization}
	Одним методів захисту, який добре себе продемонструва в ході змагань за найкращих метод атак та захисту 2018 року \cite{kurakin2018adversarial} є рандомізація вхідних даних.
	
	Зазвичай пертурбації, які генеруються в наслідок ітеративних атак, можуть легко опинитись перетренованими для деякий параметрів моделі і є унікальними для неї. Ця властивість дає можливість зробити висновок, що трансформації додані до вхідних даних, такі як зміна роміру, стискання, або додавання відступів до зобаження може зруйнувати структуру певної змагальної пертурбації і таким чином стати непоганим захистом. Якщо трансформації відбуваються випадково і нападник не знає їх характер, тоді цей метод є ефективнним як для атак на закриту моделі, так і на відкриту теж.
	Алгоритм для даного методу буде мати наступнинй вигляд:
	
	\begin{enumerate}
	\item Спочатку оригінальне зображення $x$ розмірами $W \times H \times 1$ змінюють на нове $x'$ з випадково вибраними розмірами $W' \times H' \times 1$. Зауважимо, що $|W' - W|$ та $|H' - H|$ мають бути у відносно малому проміжку, інакше точність моделі на чистих зображеннях суттєво впаде.
	\item Другим кроком буде випадковее наповнення деякого простору навколо зображення $x'$ нулями, після чого буде утворене нове зображення $x''$ з розмірами $W'' \times H'' \times 1$. Таким чином ми можемо наповнити зображення $w$ нульовими пікселями зліва, $W'' - W' - w$ нульовими пікселями справа, $h$ нульовими пікселями зверху та $H'' - H' - h$ нульовими пікселями зліва. Це в результаті дає нам  $(W''-W'+1) \times (H''- H'+1)$ різних варіації.
	\item Після двох попередніх етапів, зображення $x''$ передається оригінальному класифікатору натренованої моделі.
	\end{enumerate}

	\section{Pixel Reflection}
	Ще одним методом, який за допомогою внесення деякого шуму в зображення, намагається зруйнувати його змагальну структуру є Pixel Reflection. Він був представленний в роботі \textcite{prakash2018deflecting} і добре продемонстрував себе в захисті нейронних мереж. Ідея методу полягає в тому, щоб випадково вибрати піксель зображення і так само випадково замінити його на інший піксель в його деякому малому околі. 

	
	% ============================================ %	
	\chapter{Аналіз алгоритмів}
	Для побудови графіків з високою точністю, але оптимально з точки зору ресурсозатратності, для отримання результатів по роботі алгоритмів атак та захисту використовувались 1 тис. зображень з тестового датасету, які були нормалізовані і тому складаються зі значеннь в проміжку $[0, 1]$. В якості критерія деформації прикладу буде використовуватись норма $L_{\infty}$ яка має бути меншою за максимальне значенню пертурбації $\varepsilon$. Експеременти проводились для деякої кількості рівновіддалених значень $\varepsilon$:
	$$\varepsilon_{i}=a + i \cdot h, \quad h=\frac{b - a}{n}; \quad i=0,..,n; \quad n=30$$
	\section{Аналіз атак}
	
	\begin{figure}[!htb]
		\minipage[t]{0.49\textwidth}
		\includegraphics[width=1\textwidth]{resources/attacks_bench_8_6.png}
		\caption{Графік залежності успішності атаки від величини пертурбації. В якості нижньої межі буде виступати випадковий шум. }
		\label{fig:attacks_bench}
		\endminipage\hfill
		\minipage[t]{0.49\textwidth}
		\includegraphics[width=1\textwidth]{resources/defenses_bench_8_6.png}
		\caption{Графік залежності успішності класифікації моделі від величини пертурбації. Початкова точність моделі \\ $\sim 92\%$}
		\label{fig:defenses_bench}
		\endminipage
	\end{figure}
	Під час побудування графіку залежності успішності атак від значення пертурбації \ref{fig:attacks_bench} використовувались тільки такі тестові приклади, які коректно передбачались класифікатором $f$. У випадку графіка точності моделі \ref{fig:defenses_bench} брались вопадково обрані приклади, серед яких були і ті що хибно визначались класифікатором.
	
	Як можна побачити з графіків \ref{fig:attacks_bench} та \ref{fig:defenses_bench} класичний FGSM алгоритм відстає за ефективністю від його ітераціїних модифікацй. В той час як I-FGSM та MI-FGSM методи для даної моделі і датасету дають майже ідентичні результати. Незважаючи на те що DeepFool алгоритм націлений на утворення змагальних прикладів з мінімальним рівнем деформації оригінального зображення, він опинився найслабкішим з точки зору $L_{\infty}$ норми. Однак зазвичай, приклади утворені за допомогою цього алгоритму мають меншу евклідову норму $E$ ніж в інших розглянувих алгоритмах і, відповідно, візуально вони є теж менш пошкодженими.

	\section{Аналіз захисту}

	
	% ============================================ %	
	\chapter{Висновок} 
	\large{\textbf{Висновок}} \\
	%============================================ %

	\nocite{goodfellow2014explaining}
	\nocite{kurakin2016adversarial}
	\nocite{moosavidezfooli2015deepfool}
	\nocite{dong2017boosting}
	\nocite{dong2019benchmarking}
	\nocite{yuan2017adversarial}
	\nocite{kurakin2018adversarial}
	\nocite{naseer2018local}
	\nocite{santhanam2018defending}
	\nocite{prakash2018deflecting}
	\printbibliography[title={Бібліографія}]
	% ============================================ %
\end{document}